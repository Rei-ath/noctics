# Nano Bundle Installation Walkthrough

This walkthrough recreates the full flow a user follows when installing the
smallest Noctics distribution (`nano` scale). Every command below was executed
locally and the observed issues are documented along with mitigations.

> **Prerequisites**
> - Python 3.13 (system interpreter is fine)
> - `virtualenv` support (`python -m venv`)
> - ~1 GB free disk space for the nano bundle

## 1. Build the nano bundle

Create an isolated build environment and install PyInstaller:

```bash
python -m venv .venv-build
source .venv-build/bin/activate
pip install --upgrade pip pyinstaller
```

Rebuild the nano bundle using the already staged `nano-nox` weights (pulled from a
previous release archive):

```bash
MODEL_PATH=$(realpath dist/tmp-nano/nano-noctics/_internal/resources/models/nano-nox.gguf/nano-nox.gguf) \
  scripts/build_nano.sh
```

Key artefacts after the rebuild:
- `dist/nano-noctics/` – PyInstaller output (runtime + embedded Ollama + model)
- `dist/nano-noctics.SHA256SUMS` – internal checksums

## 2. Package the installer archive

Compress the bundle and emit a manifest the bootstrapper can consume:

```bash
python scripts/package_installer_artifacts.py \
  --dist-dir dist/nano-noctics \
  --output-dir dist \
  --slug linux-x86_64 \
  --os-name linux \
  --arch x86_64 \
  --manifest dist/nano-manifest.json \
  --archive-prefix nano-noctics \
  --version 0.1.39 \
  --build dev-sim \
  --variant nano

python - <<'PY'
from pathlib import Path
import json
manifest = Path('dist/nano-manifest.json')
data = json.loads(manifest.read_text())
data['linux-x86_64']['variants']['nano']['url'] = Path('dist/nano-noctics-linux-x86_64.tar.gz').resolve().as_uri()
manifest.write_text(json.dumps(data, indent=2) + '\n')
PY
```

Outputs:
- `dist/nano-noctics-linux-x86_64.tar.gz`
- `dist/nano-manifest.json`

## 3. Run the bootstrapper (end‑user perspective)

Install into a clean prefix (simulating a user environment):

```bash
INSTALL_ROOT=/tmp/noctics-install
BIN_ROOT=/tmp/noctics-bin
CONFIG_ROOT=/tmp/noctics-config
MEMORY_ROOT=/tmp/noctics-memory

rm -rf "$INSTALL_ROOT" "$BIN_ROOT" "$CONFIG_ROOT" "$MEMORY_ROOT"
mkdir -p "$INSTALL_ROOT" "$BIN_ROOT" "$CONFIG_ROOT" "$MEMORY_ROOT"

NOCTICS_INSTALL_HOME="$INSTALL_ROOT" \
NOCTICS_BIN_DIR="$BIN_ROOT" \
NOCTICS_CONFIG_HOME="$CONFIG_ROOT" \
NOCTICS_MEMORY_HOME="$MEMORY_ROOT" \
python installer/bootstrap.py --manifest dist/nano-manifest.json --force
```

Result:

```
Unable to detect VRAM; defaulting to 'nano' variant.
Installed Noctics runtime to /tmp/noctics-install
Launcher shim created at /tmp/noctics-bin/noctics
```

## 4. First run and debugging

Attempting to launch the CLI immediately fails:

```bash
PATH=/tmp/noctics-bin:$PATH \
NOCTICS_CONFIG_HOME=/tmp/noctics-config \
NOCTICS_MEMORY_HOME=/tmp/noctics-memory \
NOCTICS_INSTALL_HOME=/tmp/noctics-install \
NOCTICS_BIN_DIR=/tmp/noctics-bin \
noctics --help
```

Observed error:

```
Error: invalid model name
Command '['.../ollama', 'create', 'nano-nox', '-f', '.../nano-nox.modelfile']' returned non-zero exit status 1.
```

### Root cause

- The bundle now includes only the raw `nano-nox.gguf` weights.
- Ollama expects a full model manifest (digest + metadata). Without it, the CLI
  attempts to run `ollama create`, which rejects the inline modelfile with
  “invalid model name”.

### Workaround

Until we ship pre-baked Ollama manifests, the embedded runtime must fall back
to an external provider. Export an API key (or configure a local Ollama daemon)
before launching:

```bash
export OPENAI_API_KEY=sk-...
PATH=/tmp/noctics-bin:$PATH \
NOCTICS_CONFIG_HOME=/tmp/noctics-config \
NOCTICS_MEMORY_HOME=/tmp/noctics-memory \
noctics --setup  # select OpenAI and paste the key once
noctics tui
```

The CLI succeeds once a remote instrument is configured.

## 5. Orchestration smoke test

With an external provider active, the orchestration loop behaves as expected.
Example (non-interactive) check:

```bash
python -m pytest tests/test_orchestration_flow.py::test_process_instrument_result_roundtrip -q
```

(Requires `pytest` inside a venv.)

## Outstanding fixes before launch

1. **Ship full Ollama manifests** – bundle the `manifests/` and `blobs/`
   directories generated by `scripts/prepare_assets.sh` so the embedded runtime
   recognises `nano-nox` without calling `ollama create`.
2. **Rebuild bundles after `runtime_env.py` updates** – the installer must pick
   up the `OLLAMA_HOST` scheme fix when we regenerate the archives.
3. **Document remote fallback** – until the manifests ship, call out the need to
   set `OPENAI_API_KEY` (or another provider) explicitly.

Once the bundles include the manifests the bootstrapper should complete
noise-free and the local model will be usable out of the box.
